# MLP Mini

A minimal implementation of a Multi-Layer Perceptron (MLP) from scratch, demonstrating core concepts of automatic differentiation and computational graphs.

## Features
- Custom `Neuron` class implementing forward and backward propagation
- Support for basic operations (+, -, *, **)
- Computational graph visualization using Graphviz
- PyTorch implementation available

## Requirements
- Python 3.x
- NumPy
- Graphviz
- PyTorch (for alternative implementation)

## Usage
The notebook `mlp_mini.ipynb` contains two implementations:
1. Custom implementation using `Neuron` class
2. PyTorch-based implementation

Both versions include visualization of the computational graph to help understand the network structure and gradient flow.