# MLP Mini

A minimal implementation of a Multi-Layer Perceptron (MLP) from scratch, demonstrating core concepts of:
- Chain Rule
- Gradient Descent
- Backpropagation
- Loss Accumulation
- Training Cycle

## Requirements
- Python 3.x
- NumPy
- Graphviz
- PyTorch (for alternative implementation)

## Usage
The notebook `mlp_mini.ipynb` contains two implementations:
1. Custom implementation using `Neuron` , `NeuralNet`, `Model` classe
2. PyTorch-based implementation for each counterpart

Both versions include visualization of the computational graph to help understand the network structure and gradient flow.